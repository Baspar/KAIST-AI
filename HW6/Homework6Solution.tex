\documentclass{article}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{array}
\usepackage{eurosym}
\usepackage{minted}
\usepackage{float}
\usepackage{amsmath}
\usepackage{color}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[makeroom]{cancel}

\definecolor{bg}{rgb}{0.95,0.95,0.95}

\begin{document}
\title{\textbf{''Introduction to Artificial Intelligence: Homework \#6''}}
\author{LAINE Bastien \#20156441}
\date{Dec. 2th 2015}
\maketitle
\tableofcontents

\newpage
    \section{Problem 1}
        \subsection{A}
            With such a grammar, and for this given sentence, we can find 2 possible parse trees.\\
            The difference between these two is that we can consider the word ``playing'' either as a \textbf{JJ} or as a \textbf{VBZ}.
            \begin{figure}[H]
                \centering
                \includegraphics[scale = 0.5]{problem1/graph1.png}
                \caption{$1^{st} graph$}
            \end{figure}
            \begin{figure}[H]
                \centering
                \includegraphics[scale = 0.5]{problem1/graph2.png}
                \caption{$2^{nd} graph$}
            \end{figure}
        \subsection{B}
            Given the sentence and the grammar we can build the following CYK parser table:
            \[
                \begin{tabular}{r|ccccccccc}
                    l = 5 & S\\
                    l = 4 &  &\\
                    l = 3 &  &  & VP\\
                    l = 2 & NP &  &  & NP, VP\\
                    l = 1 & DT & NN & VBZ & JJ, VBG & NNS \\
                    \hline
                    & an & actress & likes & playing & kids \\
                \end{tabular}
            \]
    \newpage
    \section{Problem 2}
        \subsection{A}
            The following words are lexically ambiguous:
            \begin{description}
                \item[Struggling] which can also mean ``To exert muscular energy''
                \item[Healthy] which can also mean ``Possessing good health'' (In a medical way)
                \item[Even] which can also mean ``The contrary of odd'' (In a mathematic way)
                \item[Recover] which can also mean ``To get back (something lost or taken away), especially by making an effort:''
                \item[Real] which can also mean ``Existent''
                \item[Business] which can also mean ``Deal''
            \end{description}
        \subsection{B}
            The 2 following sentences are syntacticly ambiguous:
            \begin{description}
                \item[depriving even healthy companies of money for expansion and hiring] which can also mean that the compagnie lacks of money for expansion, and lacks also of hiring.
            \end{description}
        \subsection{C}
            The usage of ``struggling'' is a metaphor in this sentence.\\
            It compares the fact that the banks have a bad time to recover from loans to someone who is physically trying to resist against something
        \subsection{D}
            Since we have to keep the structure of the sentence, we can only use the second signification described in problem2.A.\\
            But since none of these second signification is meaningful in this sentence, we cannot find any semantic ambiguity.
    \newpage
    \section{Problem 3}
        \subsection{A}
            Lets build 2 regular expressions, one for URLs and the other for numbers:
            \begin{description}
                \item[URLs]: $([Hh][Tt][Tt][Pp][Ss]?://|[Ww]\{3\}\backslash .)([A-Za-z0-9\_ -]+\backslash .)+[A-Za-z]\{2,5\}[A-Za-z0-9/\_? = -]*$\\
                    It allows us to catch URLs which looks like ``http://XXX.com'' or ``www.XXX.com''.\\
                    The end of the regexp matches the remaining part of the URL, containing the folders, variable for PH, ect..
                \item[Phone Numbers]: $\backslash+?[0-9][0-9 -]\{3,\}[0-9]\backslash+?$\\
                    This one is really simple. We just add the cases with ``+'' at the beginning and the end of the number, and we check that the number begins with a digit, and ends with it.\\
                    In the middle, there can be a least 3 digits (+2 at both ends, which make the 5 digits for the smallest phone numbers).\\
                    We also add the possibility to have space or syphen inside the number.
            \end{description}
            On top of that, by looking at the training test case, we can see that there are a lot of price in spam cases. We will add a 3rd regexp for it.\\
            There are 2 cases: `` \$XXX,XXX.XXX'' and ``XXX,XXX.XXXp''\\
            (Important: since I'm on Linux, I have some problem with the file encoding. For this exercise, I've replace every ``£'' by a ``\$'' with another software -Vim-)
            \begin{description}
                \item[Price1]: $[$\euro£\$$][0-9]+([,\backslash.][0-9]+)?$
                \item[Price2]: $[0-9]+([,\backslash.][0-9]+)?p$
            \end{description}

            You can find below the used code:
            \begin{minted}[linenos, breaklines]{python}
def replace_regexp(string):
    #Correct file encoding
    string = re.sub(r"&amp", " & ", string)
    string = re.sub(r"&lt[; ]", "<", string)
    string = re.sub(r"&gt[; ]", ">", string)

    #Recover "previous" markers
    string = re.sub(r"<.*?>", "<DECIMAL>", string)

    #URLs
    string = re.sub(r"([Hh][Tt][Tt][Pp][Ss]?://|[Ww]{3}\.)([A-Za-z0-9_ -]+\.)+[A-Za-z]{2,5}[A-Za-z0-9/_? = -]*", "<URL>", string)

    #Phone
    string = re.sub(r"\+?[0-9][0-9 -]{3,}[0-9]\+?", "<PHONE>", string)

    #Prices
    string = re.sub(r"\$[0-9]+([,\.][0-9]+)?", "<PRICE>", string)
    string = re.sub(r"[0-9]+p", "<PRICE>", string)

    #Remove useless punctuation
    string = re.sub(r"[\.\*#\\,\?!:;\"]", " ", string)

    return string
            \end{minted}
        \subsection{B}
            \begin{minted}[linenos, breaklines]{python}
def filter_stopwords(string, stopwords):
    tweet = TweetTokenizer()
    return [x for x in tweet.tokenize(string) if x not in stopwords]
            \end{minted}
        \subsection{C}
            In order to store the vocabulary, we'll use a dictionary, and we will include the notion of ``maximum size'' with a method add\_voc(label, word) (The label will be used for Bayes classifier)
            \begin{minted}[linenos, breaklines]{python}
VOCMAX = 10000
def add_voc(label, string):
    #If we have already seen this word
    if(string in vocab.keys()):
        vocab[string]+ = 1
        category_count_word[label][string] = category_count_word[label].get(string, 0)+1
    #New word, doesn't exceed the max size of the voc
    elif(len(vocab)< = VOCMAX):
        vocab[string] = 1
        category_count_word[label][string] = 1
    #We exceed the max size of the voc
    else:
        vocab["UNKNOWN"]+ = 1
        category_count_word[label]["UNKNOWN"]+ = 1
            \end{minted}
        \subsection{D}
        This function is just using the PorterStemmer object.
            \begin{minted}[linenos, breaklines]{python}
def do_stemming(string):
    stemmer = PorterStemmer()
    return [stemmer.stem(word) for word in string]
            \end{minted}
        \subsection{E}
            This method is really straight foward.\\
            We just found again the manipulation on category\_count, like in add\_voc(label, word)
            \begin{minted}[linenos, breaklines]{python}
def train(labels, strings):
    for (label, string) in zip(labels, strings):
        category_count[label]+ = 1
        for word in string:
            add_voc(label, word)
            \end{minted}
        \subsection{F}
            To implement this method, we have to find which the greater probability between $P(ham|string)$ and $P(spam|string)$
            We will manipulate the equation a bit:
            \[
                \begin{tabular}{rcl}
                    P(label|string) & = & $\frac{P(string|label)*P(label)}{P(string)}$
                \end{tabular}
            \]
            In our case, we can remove $P(string)$, since it's a constant, and we want to find the biggest probability
            \[
                \begin{tabular}{rcl}
                    P(label|string) &$\approx$& P(string|label)*P(label)\\
                    & = & $P(label)*\prod_{word}P(word|label)$\\
                    & = & $P(label)*\prod_{word}\frac{P(word, label)}{P(word)}$\\
                \end{tabular}
            \]
            \begin{minted}[linenos, breaklines]{python}
def classify(string):
    #Preprocessing of the word
    words = do_stemming(filter_stopwords(replace_regexp(string), stopwords))

    #Initialize total probability
    p_ham_given_string = 1.
    p_spam_given_string = 1.

    for word in words:
        #If the word is not in our vocabulary, be won't consider it
        if word in vocab.keys():
            #Computation of p(word)
            p_word = float(vocab[word])/sum(vocab.values())

            #Computation of p(string, label) for both labels.
            #Method get(word, 0) return 0 if word isn't a key of the dictionnary
            p_string_and_ham = float(category_count_word["ham"].get(word, 0))/category_count["ham"]
            p_string_and_spam = float(category_count_word["spam"].get(word, 0))/category_count["spam"]

            #If one of our probability is 0, that is to say the word isn't in the category vocabulary.
            #We won't consider it
            #Else, we compute p(label|word) = p(label, word)/p(word), and multiply it to get p(label|string)
            if(p_string_and_ham>0):
                p_ham_given_string *= p_string_and_ham/p_word
            if(p_string_and_spam>0):
                p_spam_given_string *= p_string_and_spam/p_word

    #We choose the best conditional probability
    if(p_spam_given_string>p_ham_given_string):
        return "spam"
    else:
        return "ham"
            \end{minted}
        \subsection{G}
            If we quickly sort the results, we can make this array:
            \[
                \begin{tabular}{|c|c|c|}
                    \hline
                    True label & Estimated label & Number of concerned cases\\
                    \hline
                    Spam & Ham & 3\\
                    \hline
                    Ham & Spam & 54\\
                    \hline
                    Spam & Spam & 147\\
                    \hline
                    Ham & Ham & 912\\
                    \hline
                \end{tabular}
            \]
            Given so, we can compute the precision, the recall, and the f1-measure for both ham and spam categories.
            \[
                \begin{tabular}{|c|c|c|c|}
                    \hline
                    Category & Precision & Recall & f1-measure\\
                    \hline
                    ham & 0.944099378882 & 0.996721311475 & 0.969696969697\\
                    \hline
                    spam & 0.98 & 0.731343283582 & 0.837606837607\\
                    \hline
                \end{tabular}
            \]
\end{document}
