\documentclass{article}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{array}
\usepackage{minted}
\usepackage{float}
\usepackage{amsmath}
\usepackage{color}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[makeroom]{cancel}

\definecolor{bg}{rgb}{0.95,0.95,0.95}

\begin{document}
\title{\textbf{''Introduction to Artificial Intelligence: Homework \#5''}}
\author{LAINE Bastien \#20156441}
\date{Nov. 18th 2015}
\maketitle
\tableofcontents

\newpage
    \section{Problem 1}
        \subsection{A}
            \subsubsection{$p(a,b)\ne p(a).p(b)$?}
                Using the formula $p(a, b)=\sum_c p(a, b, c)$, we can compute $p(a, b)$
                \[
                    \begin{tabular}{|c|c|c|}
                        \hline
                        $a$&$b$&$p(a,b)$\\
                        \hline
                        0&0&0.336\\
                        \hline
                        0&1&0.264\\
                        \hline
                        1&0&0.256\\
                        \hline
                        1&1&0.144\\
                        \hline
                    \end{tabular}
                \]
                According to the same formula, we can draw that $p(a)=\sum_b p(a,b)$ and that $p(b)=\sum_a p(a,b)$

                \[
                    \begin{tabular}{|c|c|}
                        \hline
                        a&p(a)\\
                        \hline
                        0&0.600\\
                        \hline
                        1&0.400\\
                        \hline
                    \end{tabular}
                \]
                \[
                    \begin{tabular}{|c|c|}
                        \hline
                        b&p(b)\\
                        \hline
                        0&0.592\\
                        \hline
                        1&0.408\\
                        \hline
                    \end{tabular}
                \]
                \[
                    \begin{tabular}{|c|c|c|}
                        \hline
                        $a$&$b$&$p(a)p(b)$\\
                        \hline
                        0&0&0.355\\
                        \hline
                        0&1&0.244\\
                        \hline
                        1&0&0.236\\
                        \hline
                        1&1&0.163\\
                        \hline
                    \end{tabular}
                \]
                Given that, we can clearly saw that $p(a, b)\ne p(a).p(b)$
            \subsubsection{$p(a,b|c)=p(a|c)p(b|c)$?}
        \subsection{B}
            \subsubsection{p(a)}
            \subsubsection{p(b|c)}
            \subsubsection{p(c|a)}
            \subsubsection{p(a,b,c)=p(a)p(c|a)p(b|c)?}
                Using Bayes formula, we can easily deduct that:
                \[
                    \begin{array}{rcl}
                        p(a)p(c|a)p(b|c) &=& p(a\cap c)p(b|c)\\
                        &=& p(a\cap c\cap b)\\
                        p(a)p(c|a)p(b|c) &=& p(a, b, c)\\
                    \end{array}
                \]
                Finaly, we can say that $p(a,b,c)=p(a)p(c|a)p(b|c)$
            \subsubsection{Directed graph}
    \section{Problem 2}
        \subsection{A}
            We begin with the whole set of example.\\
            First, we compute the entropy of this step using the formula:
            \[
                \sum_{i\in\{0, 1\}} -p_i.\log_2(p_i)
            \]
            Then, we'll try to divide our set according to each input.\\
            Once done, we compute the average entropy of the 2 new set by ponderating both entropy by their Cardinality over the parent's cardinality.\\
            then, we choose the ``most interesting split'', that is to say the one whose entropy is maximum.\\

            Then, we compute again this step, but for our two new sub set. We'll stop our iteration when we found a leaf, that is to say a state where all of the ``outputs'' are the same (either 0 or 1 in our case)\\
            Since such a process is boring, I've developed a Bash script, whose goal is to create the graph. Executing it generate a PlantUML file. (Dependencie: bc)\\

            Finaly, the final result is the following:
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.5]{problem2/graph.png}
                \caption{Final decision tree}
            \end{figure}


    \section{Problem 3}
        \subsection{A}
            Lets map the four points in the $(x_1, x_1x_2)$ space
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.5]{problem3/x1x2-x1.png}
                \caption{x1-x1x2 space}
            \end{figure}
            We can saw the \textbf{maximal margin separator} (in red), whose margin is \textbf{1}.\\

            Now that we have our separator in thsi space, we will found it back in the Euclidian space.\\
            To do so, we know that:\\
            \[
                x_1*x_2=0 \Leftrightarrow\left\{
                    \begin{array}{r c l}
                        x_1 &=& 0\\
                        & OR & \\
                        x_2 &=& 0
                    \end{array}
                \right.
            \]
            Since our separator correspond to $x_1*x_2=0$, we can transform it into 2 separators in the Euclidian space, where $x_1=0$ OR $x_2=0$.\\
            On a drawing:
            \begin{figure}[H]
                \centering
                \includegraphics[scale=0.5]{problem3/x1-x2.png}
                \caption{x1-x2 space}
            \end{figure}



    \section{Problem 4}
        \subsection{A}
            Here is a copy of the sigmoid function you can find on ``problem4/mnist.py''\\
            I use the Numpy's exponential, because it allows me to calculate the exp of every term of an array.
            \begin{minted}[linenos]{python}
def sigmoid(x):
    # a sigmoid of x

    # [4.A] FILL YOUR CODE HERE
    return 1/(1+np.exp(-x))
            \end{minted}
        \subsection{B}
            Here is a copy of the propagate function you can find on ``problem4/mnist.py''\\
            \begin{minted}[linenos]{python}
def propagate(x):
    # propagate an input x to the output layer

    # [4.B] FILL YOUR CODE HERE

    #Copy data to the input layer
    np.copyto(layers[0], x)

    #Compute y for the hidden layer
    layers[1]=sigmoid(np.dot(layers[0], weights[0]))

    #Compute y for the output layer
    layers[2]=sigmoid(np.dot(layers[1], weights[1]))

    return layers[2]
            \end{minted}
        \subsection{C}
            Here is a copy of the backpropagate function you can find on ``problem4/mnist.py''\\
            \begin{minted}[linenos]{python}
def backpropagate(y_true, learning_rate):
    # backpropagate an error from the output layer and update weights

    # [4.C] FILL YOUR CODE HERE

    #Compute deltas for output layer
    outerr = layers[2]*(1-layers[2])*(y_true-layers[2]) #(1, 10)

    #Compute deltas for hidden layers
    hiddenerr = layers[1]*(1-layers[1])*(outerr.dot(weights[1].T)) #(1, 50)

    #Change hidden-output weigths
    weights[1] = weights[1] + learning_rate*layers[1].T.dot(outerr) #(50, 10)

    #Change input-hidden weigths
    weights[0] = weights[0] + learning_rate*layers[0].T.dot(hiddenerr) #(784, 50)
            \end{minted}
        \subsection{D}
            \[
                \begin{tabular}{|c|c|}
                    \hline
                    \text{Interation} & \text{Error} \\
                    \hline
                    0 & 0.537155024662484 \\
                    \hline
                    1000 & 0.149062349319850 \\
                    \hline
                    2000 & 0.124576012748911 \\
                    \hline
                    3000 & 0.111371770213344 \\
                    \hline
                    $\cdots$ & $\cdots$\\
                    \hline
                    40000 & 0.042501735109164 \\
                    \hline
                    41000 & 0.041759621834425 \\
                    \hline
                    42000 & 0.040796208555681 \\
                    \hline
                    43000 & 0.042767219131763 \\
                    \hline
                    44000 & 0.041207886314534 \\
                    \hline
                    45000 & 0.040088629855188 \\
                    \hline
                    46000 & 0.039915601152924 \\
                    \hline
                    47000 & 0.040204314440399 \\
                    \hline
                    48000 & 0.039018303138219 \\
                    \hline
                    49000 & 0.038433948782109 \\
                    \hline
                    \text{Validation data} & 0.038829476442797\\
                    \hline
                    \text{Test data} & 0.039398346519943\\
                    \hline
                \end{tabular}
            \]
        \subsection{E}
            When choosing $\eta$ we have to keep some things in mind:
            \begin{itemize}
                \item We want to maximize $\eta$, in order to make our program \textbf{converge faster}
                \item We also want to keep $\eta$ small enough, else we could be stuck in a \textbf{local minimum}
            \end{itemize}
            Given so, we try to increment $\eta$, and we can see that from $\eta=0.7$, we saw that the validiation data error increases. We will let $\eta$ be equals to 0.7\\

            Then we have to choose a size for our hidden layer:
            \begin{itemize}
                \item If the size is to small, then we will have an underfitting, that is to say our network won't be able to ``learn'' anything.
                \item If the size is to big, then we will have an overfitting, that is to say our network won't be able to ``conclude'' anything, and will end up enumerating every cases.
            \end{itemize}
            By having a size of 200 hidden nodes, it gave us a great compromise, and improves the initial error

\end{document}
